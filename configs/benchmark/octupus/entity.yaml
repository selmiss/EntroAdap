prepared_checkpoint_path: checkpoints/octopus/Llama-3.1-8B-s3-v2
output_dir: ./checkpoints/benchmark/octopus/entity-llama-3.1-8b
run_name: "entity-llama-3.1-8b"

dataset_train_file: data/benchmark/chemical_entity_recognition/train.parquet
dataset_eval_file: data/benchmark/chemical_entity_recognition/test.parquet
# Optional: Limit the number of samples for training and evaluation
# dataset_train_max_samples: 10000  # Uncomment to limit training samples
# dataset_eval_max_samples: 1000    # Uncomment to limit evaluation samples
dataset_train_split: train
dataset_test_split: test

do_train: true
do_eval: true

learning_rate: 5e-5
num_train_epochs: 10 # Training time: 1.5h
per_device_train_batch_size: 4
per_device_eval_batch_size: 4
gradient_accumulation_steps: 16

warmup_steps: 50
log_level: info

logging_steps: 10
save_strategy: "epoch"
save_steps: 1
save_total_limit: 1

eval_strategy: "epoch"
eval_steps: 1
# Enable QA metrics computation during evaluation
eval_metrics: "recognition"
# Maximum new tokens to generate for QA responses
max_new_tokens: 8

report_to: ["wandb"]
wandb_project: "octopus-benchmark"


remove_unused_columns: false

dataloader_num_workers: 8
dataloader_prefetch_factor: 4
dataloader_pin_memory: true
dataloader_persistent_workers: true

use_custom_model: true
encoder_hidden_dim: 256
modality_embedding_dim: 4096
num_fusion_blocks: 8
num_attention_heads: 32
fusion_hidden_dim: null
fusion_intermediate_dim: null
dropout: 0.1
max_seq_length: 2048

max_atoms: 30000
max_edges: 500000
skip_on_error: true

gradient_checkpointing: true
bf16: true
use_liger_kernel: false
optim: adamw_torch
adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1e-8
weight_decay: 0.01

freeze_encoder: true
freeze_llm: false
freeze_gates: false
freeze_fusion_blocks: false
freeze_projections: false

max_grad_norm: 1.0

use_peft: true
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05