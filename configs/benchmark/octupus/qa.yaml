prepared_checkpoint_path: checkpoints/octopus/Llama-3.1-8B-s3-v1_5
output_dir: ./checkpoints/benchmark/octopus/qa-llama-3.1-8b

dataset_train_file: data/benchmark/mol_qa/train_demo.parquet
dataset_eval_file: data/benchmark/mol_qa/test_demo.parquet
dataset_train_split: train
dataset_test_split: test

do_train: false
do_eval: true

learning_rate: 0
num_train_epochs: 0
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 8
max_grad_norm: 1.0
warmup_steps: 50
log_level: info

gradient_checkpointing: true
bf16: true
use_liger_kernel: false
optim: adamw_torch
adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1e-8
weight_decay: 0.01

use_peft: true
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05

logging_steps: 10
save_strategy: "no"
save_steps: 5000
save_total_limit: 3

eval_strategy: "steps"
eval_steps: 1
# Enable QA metrics computation during evaluation
eval_metrics: "qa"
# Maximum new tokens to generate for QA responses
max_new_tokens: 512

report_to: ["wandb"]
wandb_project: "octopus-benchmark"
run_name: "qa-llama-3.1-8b"

remove_unused_columns: false

dataloader_num_workers: 8
dataloader_prefetch_factor: 4
dataloader_pin_memory: true
dataloader_persistent_workers: true

use_custom_model: true
encoder_hidden_dim: 256
modality_embedding_dim: 4096
num_fusion_blocks: 8
num_attention_heads: 32
fusion_hidden_dim: null
fusion_intermediate_dim: null
dropout: 0.1
max_seq_length: 2048

max_atoms: 30000
max_edges: 500000
skip_on_error: true

freeze_encoder: true
freeze_llm: false
freeze_gates: false
freeze_fusion_blocks: false
freeze_projections: false
