prepared_checkpoint_path: checkpoints/octopus/Llama-3.1-8B-s3-v2
output_dir: ./checkpoints/benchmark/octopus/reagent-llama-3.1-8b-seq

dataset_train_file: data/benchmark/reagent_prediction/train.parquet
dataset_eval_file: data/benchmark/reagent_prediction/test.parquet
sequence_prepend_key: selfies
insert_structure_if_missing: false
skip_graph_loading: true
# Optional: Limit the number of samples for training and evaluation
# dataset_train_max_samples: 10000  # Uncomment to limit training samples
# dataset_eval_max_samples: 10    # Uncomment to limit evaluation samples
dataset_train_split: train
dataset_test_split: test

do_train: true
do_eval: true

learning_rate: 1e-4
num_train_epochs: 5
per_device_train_batch_size: 2
per_device_eval_batch_size: 2
gradient_accumulation_steps: 32
max_grad_norm: 1.0
warmup_steps: 50
log_level: info

gradient_checkpointing: true
bf16: true
use_liger_kernel: false
optim: adamw_torch
adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1e-8
weight_decay: 0.01

use_peft: true
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05

logging_steps: 10
save_strategy: "epoch"
save_steps: 1
save_total_limit: 1

eval_strategy: "epoch"
eval_steps: 1
# Enable text generation metrics computation during evaluation
eval_metrics: "molgen"
# Maximum new tokens to generate for QA responses
max_new_tokens: 512

report_to: ["wandb"]
wandb_project: "octopus-benchmark"
run_name: "reagent-llama-3.1-8b-seq"

remove_unused_columns: false

dataloader_num_workers: 8
dataloader_prefetch_factor: 4
dataloader_pin_memory: true
dataloader_persistent_workers: true

use_custom_model: true
encoder_hidden_dim: 256
modality_embedding_dim: 4096
num_fusion_blocks: 8
num_attention_heads: 32
fusion_hidden_dim: null
fusion_intermediate_dim: null
dropout: 0.1
max_seq_length: 2048

max_atoms: 30000
max_edges: 500000
skip_on_error: true

freeze_encoder: true
freeze_llm: false
freeze_gates: false
freeze_fusion_blocks: false
freeze_projections: false
