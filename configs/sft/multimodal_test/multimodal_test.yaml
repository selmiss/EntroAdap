# Multi-modal fine-tuning configuration for testing
# Model configuration
model_name_or_path: open-r1/Qwen2.5-Math-7B-RoPE-300k

# Dataset configuration
dataset_name: data/mock_train.jsonl

# Tokenizer configuration
eos_token: '<|im_end|>'

# Training hyperparameters
learning_rate: 4.0e-5
num_train_epochs: 5
max_length: 1024
per_device_train_batch_size: 2

# Optimization settings
gradient_checkpointing: true
bf16: true
use_liger_kernel: false

# LoRA/PEFT configuration
use_peft: true
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05

# DeepSpeed configuration
deepspeed: configs/deepspeed/ds_config_zero3.json

# Output configuration
output_dir: ./checkpoints/multimodal_test/Qwen2.5-Math-7B-RoPE-300k

# Logging and evaluation
logging_steps: 10
save_strategy: epoch
save_total_limit: 2

# Training strategy
eval_strategy: "no"
report_to: []

# Multi-modal configuration
use_custom_model: true
modality_vocab_size: 10000
modality_embedding_dim: 768
num_fusion_blocks: 4
num_attention_heads: 8
fusion_hidden_dim: null  # defaults to modality_embedding_dim
fusion_intermediate_dim: null  # defaults to 4 * fusion_hidden_dim
dropout: 0.1
