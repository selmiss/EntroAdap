# Fine-tuning configuration for testing with mock data
# Model configuration
model_name_or_path: open-r1/Qwen2.5-Math-7B-RoPE-300k

# Dataset configuration
dataset_name: data/mock_train.jsonl

# Tokenizer configuration
eos_token: '<|im_end|>'

# Training hyperparameters
learning_rate: 4.0e-5
num_train_epochs: 5
max_length: 1024
per_device_train_batch_size: 2

# Optimization settings
gradient_checkpointing: true
bf16: true
use_liger_kernel: true

# LoRA/PEFT configuration
use_peft: true
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05

# DeepSpeed configuration
deepspeed: configs/deepspeed/ds_config_zero3.json

# Output configuration
output_dir: .checkpoints/ft_test/Qwen2.5-Math-7B-RoPE-300k

# Logging and evaluation
logging_steps: 10
save_strategy: epoch
save_total_limit: 2

# Training strategy
eval_strategy: "no"
report_to: []

