# Multi-Modality Training with Limited Samples
# This example shows how to train on multiple datasets with sample limits

# Model configuration
model:
  hidden_dim: 256
  num_layers: 8
  dropout: 0.1
  update_coords: true
  use_layernorm: true
  num_rbf: 32
  rbf_max: 10.0
  
  # Reconstruction head parameters
  num_elements: 119
  num_dist_bins: 64
  dist_min: 0.0
  dist_max: 20.0
  
  # Loss weights
  element_weight: 1.0
  dist_weight: 1.0
  noise_weight: 1.0

# Data configuration
data:
  # Multiple datasets with different modalities
  train_data_path:
    - data/encoder/molecules/train.parquet
    - data/encoder/protein/protein_pretrain.parquet
    - data/encoder/nacid/dna_encoder_fast.parquet
    - data/encoder/nacid/rna_encoder_fast.parquet
  
  # Validation split configuration
  val_split_ratio: 0.01  # 5% validation split
  stratified_val_split: true  # Each dataset contributes proportionally to validation
  
  cache_dir: ./data/.cache
  use_modality_sampler: true
  
  # EXAMPLE: Limit samples per dataset
  # This is useful for:
  # 1. Quick testing/debugging
  # 2. Balancing datasets of different sizes
  # 3. Memory constraints
  
  # Option A: Same limit for all datasets
  # max_samples_per_dataset: 10000  # Use 10k samples from each
  
  # Option B: Different limits per dataset (matches order in train_data_path)
  max_samples_per_dataset:
    - 20000   # molecules: use 5k samples
    - 10000  # protein: use 10k samples
    - 3000   # dna: use 8k samples
    - 3000   # rna: use 7k samples
  
  # Option C: Mixed limits (null means no limit for that dataset)
  # max_samples_per_dataset:
  #   - null   # molecules: use all
  #   - 10000  # protein: use 10k
  #   - null   # dna: use all
  #   - 5000   # rna: use 5k
  
  # Masking parameters
  node_mask_prob: 0.15
  noise_std: 0.1
  use_soft_dist_targets: false
  soft_dist_sigma: 0.5

# Training configuration
training:
  output_dir: ./checkpoints/aa_encoder_multi_limited
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  learning_rate: 1.0e-4
  num_train_epochs: 5
  warmup_steps: 200
  
  # Logging and saving
  logging_steps: 10
  save_steps: 1000
  eval_steps: 100
  eval_strategy: steps
  save_strategy: steps
  load_best_model_at_end: true
  metric_for_best_model: eval_loss
  greater_is_better: false
  save_total_limit: 3
  
  # Optimization
  fp16: true
  bf16: false
  gradient_accumulation_steps: 16
  max_grad_norm: 1.0
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-8
  
  # Data loading
  dataloader_num_workers: 8
  dataloader_pin_memory: true
  dataloader_drop_last: false
  
  # Logging and reporting
  report_to: [wandb]
  run_name: formal_with_eval
  logging_dir: null
  
  # Misc
  seed: 42
  disable_tqdm: false
  remove_unused_columns: false

# Wandb configuration
wandb:
  project: AAEncoder-Pretraining
  entity: null
  mode: online  # Options: "online", "offline", "disabled"
  tags: [aa_encoder, pretraining, multi_modality, limited_samples]
  notes: Multi-modality training with per-dataset sample limits

