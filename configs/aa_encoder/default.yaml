# Masked Reconstruction Pre-training Configuration

# Model configuration
model:
  hidden_dim: 256
  num_layers: 6
  dropout: 0.1
  update_coords: true
  use_layernorm: true
  num_rbf: 32
  rbf_max: 10.0
  
  # Reconstruction head parameters
  num_elements: 119
  num_dist_bins: 64
  dist_min: 0.0
  dist_max: 20.0
  
  # Loss weights
  element_weight: 1.0
  dist_weight: 1.0
  noise_weight: 1.0

# Data configuration
data:
  train_data_path: ./data/encoder/molecules/train.parquet
  val_data_path: ./data/encoder/molecules/val.parquet
  val_split_ratio: 0.1  # Validation split ratio when auto-splitting
  cache_dir: ./data/.cache
  
  # Masking parameters
  node_mask_prob: 0.15
  noise_std: 0.1
  use_soft_dist_targets: false
  soft_dist_sigma: 0.5

# Training configuration
training:
  output_dir: ./checkpoints/aa_encoder
  per_device_train_batch_size: 128
  per_device_eval_batch_size: 128
  learning_rate: 1.0e-4
  num_train_epochs: 5
  warmup_steps: 500
  
  # Logging and saving
  logging_steps: 50
  save_steps: 10000
  eval_steps: 2000
  evaluation_strategy: steps
  save_strategy: steps
  load_best_model_at_end: true
  metric_for_best_model: eval_loss
  greater_is_better: false
  save_total_limit: 2
  
  # Optimization
  fp16: true
  bf16: false
  gradient_accumulation_steps: 1
  max_grad_norm: 1.0
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-8
  
  # Data loading
  dataloader_num_workers: 4
  dataloader_pin_memory: true
  
  # Logging and reporting
  report_to: [wandb]
  run_name: null  # Will be auto-generated if null
  logging_dir: null  # Will use output_dir/logs if null
  
  # DeepSpeed (optional)
  deepspeed: null  # e.g., configs/deepspeed/ds_config_zero2.json
  
  # Misc
  seed: 42
  disable_tqdm: false
  remove_unused_columns: false

# Wandb configuration
wandb:
  project: AAEncoder
  entity: null  # Set to your wandb entity/team name
  tags: [aa_encoder, pretraining, graph_encoder]
  notes: AA encoder pre-training for graph encoder

