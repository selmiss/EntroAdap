output_dir: ./examples/results/mol.jsonl

input_file: data/benchmark/mol_qa/test_demo.parquet

# Inference parameters
batch_size: 1  # Batch size for inference
max_new_tokens: 512  # Maximum tokens to generate
temperature: 0.0  # Sampling temperature (0 = greedy)
top_p: 1.0  # Nucleus sampling threshold

octopus_checkpoint_path: checkpoints/octopus/Llama-3.1-8B-s3-v1_5


use_peft: true  # No LoRA needed when LLM is frozen
lora_r: 16  # (ignored when use_peft: false)
lora_alpha: 32
lora_dropout: 0.05



# Octopus configuration - scaled for Llama-3.1-8B-Instruct
use_custom_model: true
encoder_hidden_dim: 256  # Match pretrained encoder checkpoint (aa_encoder_multi_limited was trained with 256)
modality_embedding_dim: 4096  # Match Llama-3.1-8B hidden_size=4096 for fusion blocks
num_fusion_blocks: 8  # Fewer blocks for faster testing
num_attention_heads: 32  # Match Llama-3.1-8B (it has 32 heads)
fusion_hidden_dim: null  # defaults to modality_embedding_dim (4096)
fusion_intermediate_dim: null  # defaults to 4 * fusion_hidden_dim (16384)
dropout: 0.1
max_seq_length: 2048  # Maximum sequence (instruction) length for multimodal training

max_atoms: 30000       # Maximum atoms per structure (null = no limit, e.g., 30000)
max_edges: 500000       # Maximum edges per structure (null = no limit, e.g., 500000)
skip_on_error: true   # Skip samples that fail to load or exceed thresholds


